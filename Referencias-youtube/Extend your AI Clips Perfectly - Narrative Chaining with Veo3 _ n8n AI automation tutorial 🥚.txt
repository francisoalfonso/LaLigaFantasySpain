(Transcrito por TurboScribe.ai. Actualizar a Ilimitado para eliminar este mensaje.)

En este video te enseñaré a extender cualquier clip sincronizadamente usando lo que llamamos narrativa de cadena con VO3 y N8n. Porque para cualquier modelo de generación de video, incluido VO3, una limitación clave es el número de segundos que el modelo puede generar. Entonces, ¿no sería genial si pudieras tomar el último frasco de cualquier clip de video que tengas y usarlo automáticamente para extender el clip a cualquier longitud que quieras, mientras mantengas la narrativa consistente? Bueno, eso es básicamente lo que hace esta automación.

Le das cualquier video de entrada, como este, decide cuántos clips quieres agregar, describe lo que sucede en cada una de esas escenas, y una vez que la automación esté hecha, extendería el video, como este ejemplo. Y también puedes usar esto para casos específicos de uso, como videos de diseño interior o real estate, para extender contenido generado por el usuario, extender B-Roll, o incluso probar tu mano con este nicho específico que está volando en TikTok. Puedes ver que este tiene 400 millones de vistas y en su repostaje ya hay como 50 millones de vistas allí, y todo lo que hacen es extender grabación real con video generado por AI, como este.

Y una vez que aprendes este sistema de AI, no solo aprenderás cómo hacer negocios narrativos, sino que también aprenderás sobre cuatro nuevas tecnologías de AI que puedes usar para poder tu automación de contenido. ¡Empecemos! Por cierto, si eres nuevo aquí, mi nombre es Jay, ejecuto mi propia agencia creativa de AI y también fundé la comunidad de Robonuggets. Tenemos varios cientos de miembros ahora que son todos creadores de AI localizados por todo el mundo.

Y aquí en Robonuggets, nuestra misión es simple, es hacer crear con AI fácil de aprender y ganar dinero sin importar su origen. Por lo que, incluso si esto es una automación bastante compleja, voy a explicarlo paso a paso para que también puedas entender la lógica y los principios de cómo establecer un sistema como este. Ahora, antes de que vayamos a este trabajo, creo que es importante entender qué es el negocio narrativo en primer lugar, porque esto puede parecer complejo, pero una vez que entiendas lo que estamos tratando de hacer, en realidad se vuelve mucho más sencillo. 

Y por cierto, este término llamado negocio narrativo, lo oí primero de uno de nuestros miembros de la comunidad, Cory, donde básicamente lo que está hablando es que si estás tratando de generar múltiples escenas en un video más largo con AI, lo que puedes hacer en el promptero es mantener un narrativo consistente, como lo que está haciendo aquí para la primera escena, que también está incluido en la segunda escena, así como en la tercera escena. Entonces, lo que sucede es que si tienes un video que es 8 segundos de largo cuando lo generas por primera vez, en realidad obtendrás este último fraseo como la entrada de imagen para el segundo clip, y hacer un video de imagen a video a través de eso. Y porque el segundo clip empieza con esta imagen, en realidad se convierte en sincronizado, y ya que mantuviste el narrativo dentro de ese promptero, la continuidad de la generación de video también sucede más naturalmente.

Entonces, por ejemplo, en ese caso no pudiste verlo, pero eso ya transición a la tercera escena, lo que sucede alrededor del marco de los 16 segundos, pero la razón por la que es tan sincronizado es porque de ese concepto de intercambio narrativo. Y así que eso es básicamente lo que estamos intentando lograr en esta automación. Y antes de que te explique cómo funciona este sistema, lo que voy a hacer es simplemente ejecutarlo una vez, para que también puedas entender cómo funciona en práctica.

Porque para este sistema, empieza con esta sección de entrada de Google Sheet, que es básicamente conectada a este template de Google Sheet que he creado. Y la razón por la que uso Google Sheet para esto, es porque con muchos de los funcionamientos que enseñamos aquí en nuestra plataforma, podrás notar que cuando generas un video, sale para ti un URL de video similar a lo que tienes aquí. Y así, la mayoría del tiempo, el último resultado de cada video generado de ti va a ser un URL como este que termina con .mp4. Entonces, por ejemplo, este, si solo copiamos esto, esto es básicamente una generación de VO3, son 8 segundos actualmente, y nuestro intento es extender esto usando la técnica de framing narrativo que es potenciada por esta automación. 

Y luego, si volvemos a nuestro Google Sheet, lo que tengo aquí son algunas entradas sobre cuántos clipes queremos agregar para este rato específico. Así que queremos extenderlo por dos clipes, que verás en un momento. La ratio de aspecto es horizontal. 

Tenemos el modelo aquí también, que estamos seleccionando como VO3 Fast, pero también puedes hacer VO3. Y luego algunos fieles de texto libres, que es el tema narrativo, solo para dar consejos a nuestro agente de AI aquí sobre lo que es el narrativo, así como cualquier solicitud especial. Así que para este, puedes ver que para la primera escena, queremos a Robin caminar y traer una cerveza, y la segunda escena es tener a Joker unirse y traer algunos clipes. 

Y luego solo tienes una columna de estatus aquí para recorrer qué tareas hacer Así que eso es todo lo que necesitas. Ahora, lo que podemos hacer es simplemente ejecutar este flujo de trabajo, y recibirá toda esa data de entrada y lo ejecutará con nuestra automación, que la inspeccionaremos en un momento. Pero una vez que haya terminado de funcionar, la tabla con la que estamos trabajando ahora está marcada como terminada, y tenemos este último video ahora disponible para nosotros también. 

Así que si previewamos eso, verás que tenemos el clip original como nuestro punto de partida, y luego en este marco de ocho segundos, ahora ya hemos transiciónado a el segundo video. Así que ahí está, tienes a Robin enviando una cerveza de Batman a Batman, y luego ahora estamos actualmente en el tercer video, con el Joker traendo algunos chips. Así que con ese input eres capaz de guiar a tu agente, que está disponible aquí en esta automación, sobre qué elementos deberían venir a seguir. 

Entonces, ¿cómo todo eso sucedió? Bueno, si miras a esto, hay en realidad un montón de nodos en esta automación, que yo mismo también me sorprendí de lo complejo que tomó para establecerlo. Pero si solo miras los pasos aquí, hay en realidad una progresión muy lógica sobre cómo se logró. Pero antes de que lleguemos a eso, voy a empezar desde el principio, para que puedas entender qué sucede en esta primera sección de input aquí. 

Y lo que tenemos aquí en el principio es solo un módulo Get Rows in Sheet, donde puedes ver que está conectado a mi página de Google, y todo lo que realmente estamos haciendo aquí es obtener el siguiente rojo donde la columna de estatus dice Para Producción. Y así, como salida de eso, solo obtuvimos ese rojo que estábamos viendo antes, específicamente ese donde le estamos pidiendo que haga estas dos escenas, ¿verdad? Así que eso es todo lo que hace ese nodo, y luego tienes este extra nodo que he añadido, que entenderás más en un momento, pero lo que básicamente hace esto es que todavía está conectado a ese módulo de Google, pero también está eliminando este módulo. Entonces, ¿qué es este módulo 2? Entonces, si volvemos a nuestro módulo de Google aquí, este tab principal que estábamos llenando en los inputs anteriormente, este es en realidad el módulo 1, pero tenemos otro módulo aquí, donde si lo inspectas, eso es básicamente las escenas que generaste y que estábamos almacenando aquí.

Entonces, por ejemplo, este URL, si clickeo en eso, eso me llevará al clip de 8 segundos, que viste antes como la primera escena extendida que agregamos a nuestro clip original. Entonces volveremos a esto más tarde, pero esencialmente este módulo de Google está aquí para que limpiemos este módulo para este rato específico. Entonces, cuando este nodo funcionó, básicamente lo que hizo fue simplemente removerlo completamente, para que estemos empezando de nuevo.

Y luego, finalmente, tienes este nodo de editar los campos, que se llama valores iniciales, y lo que básicamente está haciendo es establecer los valores iniciales para tres campos muy importantes. Empezando del fondo, tenemos este campo de URL de video, que obtuvimos a través de Google Sheets. Así que recuerda, este es nuestro primer clip de entrada en este caso.

Y luego tienes dos números aquí. Uno es el número completo, que es básicamente cuántas escenas vas a generar. Entonces, si te acuerdas, dijimos que queríamos añadir dos clips.

Y luego el paso es siempre va a empezar con el número uno. Y luego verás por qué esto es importante, porque en realidad vamos a comparar estos dos números para ver si ya hemos terminado con nuestro trabajo a mano. Entonces, ahora que tenemos esos valores iniciales, básicamente tienes este otro nodo de editar los campos, que se llama el looper, que va a tener esos mismos campos, pero debido a la forma en que esta expresión ha sido diseñada, que es json.step, lo que siempre hará es tomar el valor de su nodo precedente.

Así que puedes ver aquí, la primera vez que salió, lo que sabes porque dice uno de dos, y luego veremos este dos de dos. Lo que básicamente hizo es obtener los valores de nuestro nodo inicial aquí, que es uno, dos, así como este URL de video, que si te acuerdas es nuestro primer nodo de entrada. Y así, por la primera vez que este nodo salió, lo que básicamente sucederá es que este URL de media, que es el que pusiste en la tabla de Google, se llevará a cabo con el procesamiento que tenemos aquí.

Y vamos a ir a través de esto en más detalle, pero básicamente lo que sucede aquí es que ese video será analizado a través de estos nodos y solo describiremos lo que sucede en ese video en mucho detalle, lo que ahora envía a nuestro agente de AI, para que nuestro agente de AI pueda tener una comprensión de lo que sucede en ese video. Y la razón por la que eso es importante es porque este agente va a crear el prompt para la siguiente escena y integrar ese narrativo al próximo prompt. Pero antes de generar el video, primero tenemos que obtener el último frame de ese clip de 8 segundos que tenemos como entrada, similar a nuestro ejemplo anterior, que conseguimos aquí y que vamos a ver cómo lo hicimos en un poco.

Y luego aquí, en el paso cuatro, es donde en realidad generamos esa escena. Así que tomando ese último frame o el video precedente y usando eso como el primer frame aquí y también tomando el prompt que este agente de AI acaba de generar, ahora generamos una escena completamente nueva, que para el paso cinco es donde en realidad agregamos esa escena a Google Sheets, que si te acuerdas sería este. Y luego lo que hacemos es simplemente incrementar un paso usando este nodo de edit fields, donde tienes esos tres fields que teníamos anteriormente.

Y de nuevo, empezando desde el fondo, básicamente solo reasignamos el URL del video a un nuevo valor, que es este, que si te acuerdas es ahora la escena que acabamos de generar. Tenemos este campo para completo, que remana el mismo, porque siempre generaremos solo dos escenas de todos modos. Y luego para el campo para paso, lo que estamos haciendo aquí, si puedes ver, es que solo estamos agregando uno.

Es por eso que este nodo de edit fields se llama paso incrementado. Y así, cuando esto se ejecutó la primera vez, lo que básicamente sucedió es que ahora reemplazas tu URL de video con la siguiente escena, que lo usaremos como un input para la carrera que sucederá. Tienes el completo que permanece como dos, y luego tienes el paso incrementado de uno a dos también.

Así que esto ahora se alimentará de tu paso de éxito, que básicamente es volver a este looper y ejecutar todo de nuevo. Así que cuando lo abrimos de nuevo, y si miramos la segunda vez que se ejecutó aquí, ahora puedes ver que el input sólo se convirtió en este nuevo URL de media, que fue la generación que tuviste en la primera carrera. Así que de nuevo, si solo copiamos eso y lo mostramos en nuestro browser, verás que esta es la primera escena extendida que hicimos con Robin allí.

Y así, lo que sucedió en nuestro trabajo cuando lo ejecutamos, fue que hicimos otra carrera, porque recuerda, le pedimos extenderlo por dos escenas, pero ahora la diferencia es que la segunda vez que se ejecutó, ahora está usando esto como un input. Así que está analizando este video, estamos haciendo el próximo promto usando este video como base, y también estamos obteniendo la última escena de este video para generar la siguiente escena, lo que también agregamos a este sitio de Google. Y cuando se agregó, este fue el resultado, que si miras este video específico que generamos, ahora empieza de la última escena de la escena precedente, pero esta vez es un toque completamente fresco que seguía esa instrucción antes, con Joker apareciendo con chips.

Así que ahí está, así es como esta automación en general funciona. Así que ahora lo que voy a mostrar es cómo el procesamiento realmente sucede para cada uno de estos pasos, porque creo que para cada uno de estos, usamos tecnologías de automación de contenido completamente nuevas que pueden ayudarte con muchos de tus otros casos de uso también, así que creo que vale la pena simplemente ingresar a ellos. Y también es muy lógico cómo esto flota, porque recuerda, el primer paso es para nosotros simplemente analizar el video precedente.

Entonces, si solo miras esto, es básicamente un nodo de post-recomendación, donde estamos enviando una recomendación, que si voy aquí y expando para analizar este URL de video, que de nuevo es nuestro video de inicio, la primera vez que lo hizo, y luego solo tenemos un prompt aquí sobre lo que queremos analizar. Así que queríamos describir qué sucede en la última escena del video, asegúrate de describir la música y así más. Y solo tenemos análisis detallado como verdad, porque cuando lo intentaba, creo que es más apropiado para este caso, que verás los resultados en un momento, pero básicamente tenemos esa recomendación y donde la estamos enviando es a este modelo llamado Video Entendimiento, que es hosteado por file.ai. En file.ai nos presentamos mucho en este canal, básicamente son un agregador de modelos de AI, como una tienda de aplicaciones, y si solo buscas Video Entendimiento aquí, podrás encontrar ese modelo que estábamos usando.

Y lo que realmente hace este modelo es que solo devuelve a ti una descripción de lo que sucede en un video determinado, lo cual es útil para nuestro caso. Y así cuando este nodo salió, solo envió esa solicitud. Solo tenemos un nodo de espera aquí, porque cuando esa solicitud es enviada, toma un tiempo para analizarlo, y así solo tenemos eso como un paso, práctica bastante estándar con muchos de estos modelos.

Y luego tienes otra solicitud de HTTP para obtener la información que solo solicitaste. Y así esto es bastante estándar con file.ai, donde el análisis de video detallado se obtiene de este URL de respuesta, por eso lo conseguimos allí, y luego solo tenemos nuestro set de credenciales aquí, y cuando este nodo salió, pudimos obtener este salto, el cual puedes ver, si solo miraslo, es un análisis detallado de lo que sucede en ese video. Y así, según nuestra solicitud, solo nos dio un análisis de lo que sucede en el último frasco, y luego una descripción bastante concreta de todo lo demás que sucede en esos ocho segundos.

Y la razón por la que eso es importante es porque cuando ahora pidas a tu agente AI para hacer tu siguiente solicitud, que es nuestro paso 2, en realidad no puedes enviar el video en sí mismo hasta ahora, y así si abres esto en nuestra solicitud de usuario, y pasaremos a esto en más detalle, pero al final quería mostrar que lo que básicamente le estamos dando es esta información detallada alrededor del video precedente, para que tenga contexto de lo que sucede allí, y todo esto que vino de nuestro paso de análisis de video, básicamente. Pero puedes ver aquí, hay otras cosas que estamos enviando, ¿verdad? Entonces, en la parte superior, básicamente le estamos diciendo qué escena está creando, y la razón por la que eso es importante es porque, si te acuerdas, en nuestras solicitudes especiales le estábamos pidiendo que para la escena 1 es donde Robin debería entrar, y para la escena 2 es donde Joker debería entrar, y así el contexto para romper esas escenas aparte es dado aquí en este campo que hemos llenado en nuestras solicitudes de Google antes. Y luego solo tenemos un par de campos dinámicos aquí alrededor del aspecto preferido, el modelo preferido, así como el tema narrativo preferido, que si miras esto, todo eso solo vino de nuestro nodo de nuestras solicitudes de Google en este nodo de ingresos de Get desde el primer paso.

Así que ese es el paso de usuario, y ahora el mensaje de sistema, si abres eso, esto es bastante detallado, que puedes leer en tu tiempo propio, pero esencialmente sólo proporciona guía sobre lo que la tarea de mano es básicamente resumirlo como un agente de AI de continuación de escena y que debería generar instrucciones o promtes de generación de video, y luego solo incluí un par de cosas aquí para la guía, donde debería considerar idealmente la consistencia con el estilo de la música y así sucesivamente. Que el enfoque sea lo que sucede en el último frasco del video, que también incluimos en nuestra solicitud de usuario. Solo algunas consideraciones de calidad como definir el tipo de cámara que se utiliza, el movimiento de la cámara que debería ser consistente con la escena anterior, y así sucesivamente.

Así que puedes leer eso en tu tiempo propio y también editarlo si deseas. Así que si tienes un caso específico de uso o una marca con la que estás trabajando, te recomiendo que logres el estilo que quieras, esto es donde deberías empezar a refinarlo. Porque todos nuestros inputs aquí resultarían en este resultado, donde tenemos el promte de video que estamos a punto de pasar a VideoTree.

Así que puedes ver aquí, Robin vestido en un costumbre colorido entra al cuarto y tal. Ese es nuestro promte de video. Tienes la ratio de aspecto, que es según nuestra definición, dijimos horizontal y dijimos que queremos VideoTree rápido.

Y solo para mostrarlo también, si voy a RUN 2 of 2, esta es la parte en la que el promte se convirtió en Joker. Ahora es el que entra al cuarto también, porque esa es la segunda escena extendida que queremos en este WhoWillRun. Así que ahora tenemos el promte.

Antes de que podamos ir a generar el video, en realidad también necesitamos obtener el último frame del video anterior. Porque recuerda, vamos a enviar eso a este paso para que tengamos esa propiedad de semejanza entre escenas. Y así, para hacerlo, si miro a esta solicitud de HTTP, estamos usando File.ai nuevamente, pero específicamente esta función ExtractFrame.

Y así, si vuelves a File y solo buscas ExtractFrame, lo encontrarás. Y todo lo que hace este punto final es que te permite extraer la primera, la mitad o el último frame de cualquier video, lo que en realidad es bastante útil. Así que si vas abajo y miras nuestra solicitud aquí, es una solicitud muy simple donde pasamos por el URL de video, donde estamos diciendo que queremos el último frame de este video.

Y así, cuando eso se pasó, File.ai dice que nuestra solicitud está en línea. Una vez más, esperamos unos segundos para que termine. Y una vez que esté hecho, solo obtenemos los resultados de nuestra solicitud.

Así que esto es lo mismo, lo estamos obteniendo de este URL de respuesta según la documentación de File.ai. Y luego, para la primera fila, recuperamos esta imagen, que es una JPEG, que si miras eso, este es el frame final de nuestro video original, si te acuerdas. Y luego, si vas a la segunda fila, lo que debería mostrar ahora es una imagen con Robin allí. Así que esto va a ser el último frame de nuestra primera escena extendida.

Y así, esto actúa como una especie de cadena para extender estos clipes para que cuando los unimos, serían bastante sincronizados. Pero ahí vamos, ahora tienes tu imagen para tu imagen del paso de video y tienes el prompto. Así que tienes todo lo que necesitas para actualmente generar el próximo clip.

Y así, este paso cuatro es justo alrededor de eso, donde estamos haciendo otra solicitud de HTTP. Y si abres este, es una solicitud de posta para VEO como modelo, pero esta vez estamos usando Key.ai. Así que ya hemos hablado de Key.ai anteriormente en nuestros videos más antiguos, pero también son un agregador de modelos de AI similar a File.ai. Pero la razón por la que los estamos usando es porque, ahora mismo, en realidad tienen la implementación más barata de VEO3 Fast. Así que es sólo 40 centavos para un video de 80 segundos, lo cual es muy económico.

Y así, su servicio es lo que estamos usando en este caso. Y si miras el cuerpo, esto es bastante estándar, al menos para Key.ai, donde estamos pasando el prompto. Así que puedes ver aquí, al menos en este preview, ya estamos obteniendo la segunda escena extendida.

Es por eso que hay Joker allí. Desafortunadamente, no creo que pueda moverme a la primera generación con Robin, pero el formato sería casi lo mismo. Tienes el modelo, VEO3 Fast, el ratio de aspecto y el url de imagen.

Así que esto es importante porque estamos pasando por este url de imagen para ser el punto de partida de esta escena que estamos a punto de generar. Así que cuando se ejecute, dice suceso, lo que significa que ha sido publicado. Tenemos otro nodo de espera aquí solo para esperar a que esa generación termine.

Y luego tienes tu GetVideo, que es un poco diferente, solo según la documentación de KAI y cómo funciona. Pero básicamente estamos obteniendo nuestra generación viniendo de este ID de tareas, lo cual es único y fue asignado a nosotros cuando esa tarea de generación fue creada para los servidores de KAI. Y puedes ver aquí, hay en realidad un par de pasos, ¿verdad? Así que si vas al primer paso, lo que probablemente sucedió aquí es que el trabajo no se ha hecho aún.

Y así puedes ver aquí que el flan de éxito todavía es cero, lo que significa que todavía está generando. Así que para ese primer instante en el que salió, si solo muevo esto hacia abajo, la razón por la que tenemos un nodo de espera aquí es para capturar eso, lo cual estamos simplemente comprobando si ese flan de éxito ya es igual a uno, lo cual sabemos para el primer paso no es verdad, por eso se mapeó a la parte falsa. Y cuando se mapeó a la parte falsa, solo esperamos un poco más.

Si solo trazas la línea roja aquí, eso es básicamente lo que es. Y así, si voy al paso 2 o 4 aquí, este es probablemente el donde el flan de éxito ya es uno y sabemos que ya está hecho porque de hecho ya tenemos el video de MP4 aquí, lo cual si solo previewamos eso, eso nos llevará nuevamente a ese video con Robin entrando con una taza de cerveza. Y así, una vez que tuviste eso, si vas a este flan de éxito de nuevo y miras el paso 2 o 4, esto ahora está mapeado a verdad porque ya ha sido sucesivo en generar.

Y así, cuando se mapea a verdad, va a este siguiente paso, que ya cubrimos antes, si solo trazas eso de vuelta, donde básicamente estamos logrando esa escena que solo generamos en nuestro sitio Google en el sitio 2, que si recuerdas sería este, se llenó por la acción de este nodo, y luego incrementamos un paso y finalmente tenemos otro nodo de si aquí, donde simplemente estamos revisando si ese valor numérico completo es menos que el paso. Así que si miro en el primer instante de esto, puedes ver que se mapea a falso, lo cual básicamente dice a la automación que, hey, todavía no hemos terminado, vamos a volver y hacer esto de nuevo, porque en el primer paso, recuerda que solo extendimos la escena por un incremento o una escena extra, pero el usuario quiere hacerlo también, y así cuando se mapea a la branca falsa, lo que realmente sucedió allí es que se volvió todo el camino hacia el campo de edición de looping, pero al mismo tiempo se lleva con él la escena que acabas de generar, para que lo utilices como punto de partida para este segundo paso, cuando se gira de nuevo, pero para ese segundo paso, una vez que llega a este nodo de si completo de nuevo, lo que realmente sucedió allí para nuestro paso 2 de 2 es que ahora el valor completo, que es el número de escenas que queremos generar, ya es menos que este valor de pasos, por lo que lo mapeamos como verdad, lo que dice a la automación que bien, ahora terminamos generando estas escenas, por lo que procedemos a este último paso, y este último paso, si puedes ver, la funcionalidad de esto es simplemente combinar esos clips que generamos en un solo video, para que no tengas que hacerlo tú mismo, y todo lo que realmente estamos haciendo aquí es obtener las escenas de nuestro papel 2, si te acuerdas, que es el propósito de por qué logramos todo en este papel extra en primer lugar, porque lo que estamos haciendo aquí es conectar este nodo a nuestro papel de Google en el papel 2, y estamos obteniendo específicamente esta columna C, porque ahí es donde están los clips de video y así cuando esto salió, puedes ver que la salida es básicamente esas dos escenas que acabamos de generar, para que podamos lograrlos de vuelta a N8n, solo necesitamos añadir este nodo de agregación, porque para que realmente lo combinemos, necesitan existir como un objeto en grupo, pero actualmente la forma en que los nodos de Google funcionan en N8n es que en realidad están siendo agarrados como dos objetos, lo cual no realmente funciona para nosotros, y así que todo lo que realmente hicimos aquí es agregar el campo CURL, que solo agarramos de esa manera, y cuando ese nodo salió, lo que básicamente hizo es unir esos CURLs juntos, lo cual nos permite hacer este último paso, es básicamente otro nodo de solicitud de HTTP, nuevamente a FileAI, pero esta vez estamos llamando a esta función de videos de merge, lo cual nuevamente, si vamos a FileAI y buscamos video de merge, lo verás aquí, es bastante nuevo, y todo lo que realmente hace es que si tienes un par de videos que quieres unir juntos, esta es una manera agradable y fácil para que lo hagas, y así que si volvemos a nuestro N8n instante, miramos la solicitud que acabamos de enviar, todo lo que realmente tenemos aquí es tener esos dos clipes extendidos declarados aquí, así que si solo zoomamos allí, puedes ver que este es el primer clip extendido que hicimos, y luego este es el tercero, pero por supuesto, el primer clip tendrá que estar allí, y así que solo lo incluimos aquí también, así que este es el que solo tiene Batman, este es el que tiene Batman y Robin, este es el que tiene Batman, Robin y Joker, y esta expresión es cómo lograr esto, pero una vez que salió, esencialmente, solo robó a File.ai para activar su función Merge Videos, y ahora está en línea, y así que solo esperamos un poco para que eso funcione de nuevo, y una vez que eso esté hecho, obtenemos el video final aquí, nuevamente, viniendo del URL de respuesta que nos dio File.ai, y dentro de la salida que ahora contiene tu final MP4 file, y para el cual tenemos este Update Log Google Sheet Node que ahora está conectado a nuestro Sheet 1, que está aquí, y donde la función de este nodo final que es similar a muchas de nuestras funciones es marcar este ROW como hecho y incluir el URL final para ser logado aquí, y esa es la automación ahora completa. Ahora, con todos esos servicios que acabamos de llamar, podrías estar curioso sobre el costo, pero una buena noticia con esta función es que muchos de esos pasos extra que incluimos son en realidad muy baratos, por ejemplo, con el paso de analizar el video, es sólo alrededor de 1.5 centavos por clip de 8 segundos, obteniendo el último frame es como 2 centavos de 1 centavo, así que es casi cero, y luego combinar esos clips juntos es también muy barato, porque se hace a través de FFmpeg, que es open source de todos modos, y así que el único costo variable que tienes aquí es la generación VO3, que si estás usando VO3 Fast a través de KAI es sólo 40 centavos, así que es muy económico, y así que para cada escena que extiendas, sólo serán alrededor de 42 centavos.

Otra cosa que quería mencionar es que si estás buscando hacer el mismo trabajo, pero para tu instancia tendrás la opción de usar el Toolkit de NCA, con el que puedes mirar este video de VALEX, quien es un miembro de nuestra comunidad, sobre cómo hacerlo, así que voy a enlazar su video allí en la descripción también, pero ahí está, espero que hayas aprendido algo nuevo de este video, creo que esta automación en particular sólo tendría casos de uso específicos en los que es aplicable, por ejemplo, como VALEX mostró aquí en su video, si tu cliente está en real estado o diseño interior, esto es de hecho bastante útil como automación si estás tratando de crear videos de B-Roll o de real estado en masa y extendiendo eso también, pero incluso si no estás extendiendo ningún video tú mismo en el mientraso, creo que conociendo muchos de estos servicios en los que estamos automando mucho de la edición de video, como obtener el último frame, analizar el video en sí, es una habilidad bastante útil que puedes intercambiar para cualquier otro caso de uso que puedas tener, así que si aprendes algo nuevo, considera suscribirte si aún no lo has hecho, así que podemos poner más contenido en nuestra plataforma como esta, y si quieres agarrar este template para un corto como el guía de configuración config, también puedes ver nuestra comunidad, que solo he enlazado abajo, mucha gente se unió por la riqueza de cursos que tenemos alrededor de Inteligencia Artificial y la creación con Inteligencia Artificial, pero mucha gente se unió por la comunidad que pudimos construir, así que puedes ver aquí muchas de las ganancias que la gente está publicando y también muchas oportunidades pagadas si quieres empezar con ganar de esta habilidad que estás aprendiendo con Inteligencia Artificial y automación, así que mira si eso es para ti, especialmente si quieres invertir en ti mismo y en tu aprendizaje cuando se trata de este cambio histórico que la Inteligencia Artificial está aportando, pero eso es todo por este video, los veo la próxima vez, gracias.

(Transcrito por TurboScribe.ai. Actualizar a Ilimitado para eliminar este mensaje.)